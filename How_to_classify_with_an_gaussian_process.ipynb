{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv('apple_dataset/apple_quality.csv')\n",
    "# drop the first colum\n",
    "data=data.drop('A_id',axis=1)\n",
    "#drop last row\n",
    "data=data.drop(data.index[-1])\n",
    "#all the columns but the last one is input \n",
    "X=data.iloc[:,:-1]\n",
    "#the last column is the output\n",
    "y=data.iloc[:,-1]\n",
    "\n",
    "X=X.to_numpy()\n",
    "y=y.to_numpy()\n",
    "y[y=='good']=1\n",
    "y[y=='bad']=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add noise to check if the automatic relevance determination is able to find only the relevant features to predict the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate another column of random number to the input usgin gaussian distribution\n",
    "X=np.concatenate((X,np.random.normal(size=(X.shape[0],1))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, ard=False):\n",
    "\n",
    "        if ard:\n",
    "            ard_num_dim=inducing_points.size(-1)\n",
    "        else:\n",
    "            ard_num_dim=None\n",
    "\n",
    "\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "\n",
    "        # If you want to use different hyperparameters for each task,\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(ard_num_dims=ard_num_dim, nu=2.5))\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        \n",
    "#%%\n",
    "class Classifier():\n",
    "\n",
    "    def __init__(self, X, y, num_inducing=50):\n",
    "        #pick the inducing from the training set \n",
    "        X=X.astype(float)\n",
    "        y=y.astype(float)\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.Y = torch.from_numpy(y)\n",
    "        self.inducing_points = self.X[:num_inducing]\n",
    "        self.create_model()\n",
    "        self.move_model_to_cuda()\n",
    "        \n",
    "    def create_model(self):        \n",
    "        self.model = GPModel( inducing_points=self.inducing_points, ard=True)\n",
    "        self.likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "\n",
    "    def move_model_to_cuda(self):\n",
    "        self.model=self.model.cuda()\n",
    "        self.likelihood=self.likelihood.cuda()\n",
    "        self.inducing_points=self.inducing_points.cuda()\n",
    "        self.X=self.X.cuda()\n",
    "        self.Y=self.Y.cuda()\n",
    "        self.model=self.model.double()\n",
    "        self.likelihood=self.likelihood.double()\n",
    "        self.inducing_points=self.inducing_points.double()\n",
    "\n",
    "    def train(self, num_epochs=3):\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': self.model.parameters()},\n",
    "            {'params': self.likelihood.parameters()},\n",
    "        ], lr=0.01)\n",
    "\n",
    "        # Our loss object. We're using the VariationalELBO\n",
    "        mll = gpytorch.mlls.VariationalELBO(self.likelihood, self.model, num_data=self.Y.size(-1))\n",
    "\n",
    "        train_dataset = TensorDataset(self.X, self.Y)\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "        # epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "        epochs_iter = tqdm(range(num_epochs))\n",
    "        for i in epochs_iter:\n",
    "            # Within each iteration, we will go over each minibatch of data\n",
    "            minibatch_iter = tqdm(self.train_loader, desc=\"Minibatch\", leave=False)\n",
    "            for x_batch, y_batch in minibatch_iter:\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(x_batch)\n",
    "                loss = -mll(output, y_batch)\n",
    "                minibatch_iter.set_postfix(loss=loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        print(\"Training finished\")\n",
    "        print(\"Lengthscale: \")        \n",
    "        print(self.model.covar_module.base_kernel.lengthscale)\n",
    "        print(\"Outputscale: \")\n",
    "        print(torch.sqrt(self.model.covar_module.outputscale))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "        X=X.astype(float)\n",
    "        X = torch.from_numpy(X).cuda().double()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            observed_pred = self.likelihood(self.model(X))\n",
    "            return observed_pred.mean.cpu().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Minibatch:   1%|▏         | 4/320 [00:00<00:07, 39.94it/s, loss=0.902]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:36<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n",
      "Lengthscale: \n",
      "tensor([[ 4.6167,  5.4072,  5.3381,  4.7884,  5.3512,  3.0080,  6.3960, 16.1792]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<SoftplusBackward0>)\n",
      "Outputscale: \n",
      "tensor(1.6097, device='cuda:0', dtype=torch.float64, grad_fn=<SqrtBackward0>)\n",
      "accuracy: 0.90375\n"
     ]
    }
   ],
   "source": [
    "# slip the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "svgp=Classifier(num_inducing=200, X=X_train, y=y_train)\n",
    "\n",
    "svgp.train(num_epochs=20) \n",
    "\n",
    "y_pred=svgp.predict(X_test)\n",
    "#compute accuracy\n",
    "y_pred[y_pred>0.5]=1\n",
    "y_pred[y_pred<=0.5]=0\n",
    "accuracy=(y_pred==y_test).sum()/len(y_test)\n",
    "print('accuracy:',accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
