{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "happy-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.linalg as linalg\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "class PILCO:\n",
    "    def __init__(self, X, Y, lengthscales, variance, noise, batch_size):\n",
    "        self.X = torch.tensor(X)\n",
    "        self.Y = torch.tensor(Y)\n",
    "        self.lengthscales = torch.tensor(lengthscales).float()\n",
    "        self.variance = torch.tensor(variance)\n",
    "        self.noise = torch.tensor(noise)\n",
    "        self.num_outputs = Y.shape[1]\n",
    "        self.num_dims = X.shape[1]\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def predict_on_noisy_inputs(self, m, s):\n",
    "        iK, beta = self.calculate_factorizations()\n",
    "        return self.predict_given_factorizations(m, s, iK, beta)\n",
    "\n",
    "    def calculate_factorizations(self):\n",
    "        K = self.K(self.X)\n",
    "        batched_eye = torch.eye(self.num_dims, dtype=torch.float64).repeat(self.num_outputs, 1, 1)\n",
    "        batched_eye = batched_eye.unsqueeze(0).repeat(self.batch_size, 1, 1, 1)\n",
    "        L = linalg.cholesky(K + self.noise[:, None, None] * batched_eye)\n",
    "        iK = torch.cholesky_solve(batched_eye, L)\n",
    "        Y_ = self.Y[:, :, None]\n",
    "        beta = torch.cholesky_solve(Y_, L)[:, :, 0]\n",
    "        return iK, beta\n",
    "\n",
    "    def predict_given_factorizations(self, m, s, iK, beta):\n",
    "        s = s.repeat(self.num_outputs, self.num_outputs, 1, 1).float()\n",
    "        inp = self.centralized_input(m).repeat(self.num_outputs, 1, 1).float()\n",
    "        print(inp.shape)\n",
    "        iL = (torch.diag(1 / self.lengthscales)).float()\n",
    "        iN = inp @ iL\n",
    "        B = iL @ s[0, ...] @ iL + torch.eye(self.num_dims, dtype=torch.float)\n",
    "\n",
    "        t = (linalg.solve(B, iN.transpose(-2, -1).transpose(-1, -3)).transpose(-1, -3)).transpose(-1, -2)\n",
    "        lb = (torch.exp(-torch.sum(iN * t, -1) / 2) * beta).float()\n",
    "        tiL = t.float() @ iL.float()\n",
    "        c = (self.variance / torch.sqrt(torch.det(B))).float()\n",
    "\n",
    "        M = torch.sum(lb, -1) * c[:, None]\n",
    "        V = tiL @ lb[..., None] * c[..., None]\n",
    "\n",
    "        R = s @ torch.diag(1 / self.lengthscales ** 2) + torch.eye(self.num_dims, dtype=torch.float)\n",
    "        X = inp[:, :, :] / self.lengthscales[:, None, None] ** 2\n",
    "#         print(X.shape)\n",
    "        X2 = -inp[:, :, :] / self.lengthscales[:, None, None] ** 2\n",
    "        Q = linalg.solve(R, s) / 2\n",
    "        Xs = torch.sum(X @ Q * X, -1)\n",
    "        X2s = torch.sum(X2 @ Q * X2, -1)\n",
    "        maha = -2 * (X @ Q) @ X2.transpose(-2, -1) + Xs[..., :, None] + X2s[..., None, :]\n",
    "\n",
    "        k = torch.log(self.variance)[:, None] - torch.sum(iN ** 2, -1) / 2\n",
    "        L = torch.exp(k[:, None, :, None] + k[None, :, None, :] + maha)\n",
    "        S = (beta[:, None, None, :] @ L @ beta[None, :, :, None]).squeeze(dim=-1).squeeze(dim=-1)\n",
    "\n",
    "        diagL = torch.diagonal(L.transpose(-1, -2), dim1=-2, dim2=-1).transpose(-1, -2)\n",
    "        S = S - torch.diagonal(iK @ diagL @ iK.transpose(-1, -2), dim1=-2, dim2=-1).transpose(-1, -2)\n",
    "        S = S / torch.sqrt(torch.det(R))\n",
    "        S = S + torch.diag(self.variance)\n",
    "        S = S - M @ M.transpose(-1, -2)\n",
    "\n",
    "        return M.transpose(-1, -2), S, V.transpose(-1, -2)\n",
    "\n",
    "    def K(self, x1, x2=None):\n",
    "        # Kernel function\n",
    "        if x2 is None:\n",
    "            x2 = x1\n",
    "        dist_sq = torch.cdist(x1, x2, p=2, compute_mode=\"donot_use_mm_for_euclid_dist\")\n",
    "        K = self.variance * torch.exp(-0.5 * dist_sq / self.lengthscales ** 2)\n",
    "        return K\n",
    "\n",
    "    def centralized_input(self, x):\n",
    "        # Centralize the input by subtracting the training set mean\n",
    "        return x - torch.mean(self.X, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fatal-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [2, 2] but got: [2, 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-82ca278da16a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprediction_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_noisy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction Mean:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction Covariance:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_cov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-817257bf5d48>\u001b[0m in \u001b[0;36mpredict_on_noisy_inputs\u001b[0;34m(self, m, s)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_noisy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0miK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_factorizations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_given_factorizations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_factorizations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-817257bf5d48>\u001b[0m in \u001b[0;36mpredict_given_factorizations\u001b[0;34m(self, m, s, iK, beta)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdiagL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miK\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdiagL\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0miK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [2, 2] but got: [2, 1]."
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "X = np.array([[0.0, 0.0], [1.0, 1.0]])\n",
    "Y = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "lengthscales = np.array([0.5, 0.5])\n",
    "variance = np.array([1.0])\n",
    "noise = np.array([0.1])\n",
    "\n",
    "model = PILCO(X, Y, lengthscales, variance, noise, 1)\n",
    "m = torch.tensor([0.5, 0.5])\n",
    "s = torch.tensor([[0.1, 0.2], [0.2, 0.3]])\n",
    "\n",
    "prediction_mean, prediction_cov, prediction_var = model.predict_on_noisy_inputs(m, s)\n",
    "print(\"Prediction Mean:\", prediction_mean)\n",
    "print(\"Prediction Covariance:\", prediction_cov)\n",
    "print(\"Prediction Variance:\", prediction_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "stuck-victoria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lengthscales[:, None, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-newton",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_torch",
   "language": "python",
   "name": "v_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
